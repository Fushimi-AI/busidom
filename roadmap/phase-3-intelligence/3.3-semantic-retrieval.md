# Feature 3.3: Semantic Memory Retrieval

## Metadata
| Field | Value |
|-------|-------|
| **Feature ID** | 3.3 |
| **Phase** | 3 - Intelligence |
| **Priority** | Critical |
| **Estimated Effort** | 5-6 hours |
| **Dependencies** | 3.1 (pgvector), 3.2 (Embedding Generation) |
| **Approval** | [ ] |
| **Status** | Not Started |

---

## Overview

Implement semantic memory retrieval that finds relevant past conversations and context based on the current query, injecting them into the AI context for informed responses.

## User Story

As a **founder**, I want the **AI to recall relevant past discussions** so that **advice builds on our history together**.

---

## Requirements

### Functional Requirements
1. Find similar messages by query
2. Retrieve relevant context
3. Inject retrieved memory into prompt
4. Filter by recency and relevance
5. Deduplicate similar results
6. /recall command for manual search

### Non-Functional Requirements
- Retrieval < 200ms
- Top 5-10 relevant memories
- Balance relevance vs recency
- Token-aware truncation

---

## Technical Specification

### Memory Retriever

```typescript
// src/core/memory/memory-retriever.ts
import { EmbeddingService } from '../../infrastructure/embedding/embedding.service';
import { VectorService } from '../vector/vector.service';
import { Message } from '../../types';

export interface RetrievedMemory {
  content: string;
  sourceType: 'message' | 'context';
  similarity: number;
  timestamp?: Date;
}

export interface RetrievalOptions {
  limit?: number;
  threshold?: number;
  includeContext?: boolean;
  recencyBoost?: boolean;
  maxTokens?: number;
}

export class MemoryRetriever {
  constructor(
    private embeddingService: EmbeddingService,
    private vectorService: VectorService
  ) {}

  async retrieve(
    query: string,
    options: RetrievalOptions = {}
  ): Promise<RetrievedMemory[]> {
    const {
      limit = 10,
      threshold = 0.7,
      includeContext = true,
      recencyBoost = true,
      maxTokens = 2000,
    } = options;

    // Generate query embedding
    const queryEmbedding = await this.embeddingService.embed(query);

    // Search for similar content
    const results = await this.vectorService.search(queryEmbedding, {
      limit: limit * 2, // Get more for filtering
      threshold,
    });

    // Filter and score
    let memories: RetrievedMemory[] = results
      .filter(r => includeContext || r.sourceType === 'message')
      .map(r => ({
        content: r.content,
        sourceType: r.sourceType as 'message' | 'context',
        similarity: r.similarity,
      }));

    // Apply recency boost if available
    if (recencyBoost) {
      memories = this.applyRecencyBoost(memories);
    }

    // Deduplicate similar content
    memories = this.deduplicate(memories);

    // Sort by combined score
    memories.sort((a, b) => b.similarity - a.similarity);

    // Take top results within token limit
    return this.truncateToTokenLimit(memories, limit, maxTokens);
  }

  async retrieveForContext(
    currentMessages: Message[],
    options?: RetrievalOptions
  ): Promise<string> {
    // Use last few messages as query
    const recentContent = currentMessages
      .slice(-3)
      .map(m => m.content)
      .join(' ');

    const memories = await this.retrieve(recentContent, options);

    if (memories.length === 0) {
      return '';
    }

    return this.formatMemoriesForPrompt(memories);
  }

  private applyRecencyBoost(memories: RetrievedMemory[]): RetrievedMemory[] {
    // Context always gets slight boost
    return memories.map(m => ({
      ...m,
      similarity: m.sourceType === 'context'
        ? m.similarity * 1.1
        : m.similarity,
    }));
  }

  private deduplicate(memories: RetrievedMemory[]): RetrievedMemory[] {
    const seen = new Set<string>();
    return memories.filter(m => {
      // Simple dedup by content prefix
      const key = m.content.slice(0, 100).toLowerCase();
      if (seen.has(key)) return false;
      seen.add(key);
      return true;
    });
  }

  private truncateToTokenLimit(
    memories: RetrievedMemory[],
    limit: number,
    maxTokens: number
  ): RetrievedMemory[] {
    const result: RetrievedMemory[] = [];
    let totalTokens = 0;

    for (const memory of memories) {
      if (result.length >= limit) break;

      const tokens = this.estimateTokens(memory.content);
      if (totalTokens + tokens > maxTokens) break;

      result.push(memory);
      totalTokens += tokens;
    }

    return result;
  }

  private estimateTokens(text: string): number {
    // Rough estimate: 1 token â‰ˆ 4 characters
    return Math.ceil(text.length / 4);
  }

  private formatMemoriesForPrompt(memories: RetrievedMemory[]): string {
    const contextMemories = memories.filter(m => m.sourceType === 'context');
    const messageMemories = memories.filter(m => m.sourceType === 'message');

    let prompt = '';

    if (contextMemories.length > 0) {
      prompt += '## Relevant Business Context\n';
      contextMemories.forEach(m => {
        prompt += `- ${m.content}\n`;
      });
      prompt += '\n';
    }

    if (messageMemories.length > 0) {
      prompt += '## Relevant Past Discussions\n';
      messageMemories.forEach(m => {
        prompt += `- "${m.content.slice(0, 200)}${m.content.length > 200 ? '...' : ''}"\n`;
      });
      prompt += '\n';
    }

    return prompt;
  }
}
```

### Enhanced Chat Service

```typescript
// src/core/chat/chat-with-memory.service.ts
import { ChatService } from './chat.service';
import { MemoryRetriever } from '../memory/memory-retriever';
import { Message, Result } from '../../types';

export class ChatWithMemoryService {
  constructor(
    private chatService: ChatService,
    private memoryRetriever: MemoryRetriever,
    private systemPrompt: string
  ) {}

  async send(userMessage: string): Promise<Result<Message>> {
    // Get current conversation
    const messages = this.chatService.getHistory();

    // Retrieve relevant memories
    const memoryContext = await this.memoryRetriever.retrieveForContext([
      ...messages,
      { role: 'user', content: userMessage } as Message,
    ]);

    // Build enhanced system prompt
    const enhancedPrompt = memoryContext
      ? `${this.systemPrompt}\n\n${memoryContext}`
      : this.systemPrompt;

    // Update system prompt temporarily
    this.chatService.setSystemPrompt(enhancedPrompt);

    // Send message
    const result = await this.chatService.send(userMessage);

    // Restore original prompt
    this.chatService.setSystemPrompt(this.systemPrompt);

    return result;
  }

  async *stream(userMessage: string): AsyncIterable<string> {
    const messages = this.chatService.getHistory();

    const memoryContext = await this.memoryRetriever.retrieveForContext([
      ...messages,
      { role: 'user', content: userMessage } as Message,
    ]);

    const enhancedPrompt = memoryContext
      ? `${this.systemPrompt}\n\n${memoryContext}`
      : this.systemPrompt;

    this.chatService.setSystemPrompt(enhancedPrompt);

    yield* this.chatService.stream(userMessage);

    this.chatService.setSystemPrompt(this.systemPrompt);
  }
}
```

### Recall Command

```typescript
// src/cli/commands/recall.command.ts
import chalk from 'chalk';
import { MemoryRetriever } from '../../core/memory/memory-retriever';

export function registerRecallCommand(repl: REPL, retriever: MemoryRetriever): void {
  repl.registerCommand('/recall', async (query: string) => {
    if (!query.trim()) {
      console.log(chalk.yellow('Usage: /recall <search query>'));
      return;
    }

    console.log(chalk.gray('Searching memories...\n'));

    const memories = await retriever.retrieve(query, {
      limit: 5,
      threshold: 0.6,
    });

    if (memories.length === 0) {
      console.log(chalk.yellow('No relevant memories found.'));
      return;
    }

    console.log(chalk.bold('ðŸ“š Retrieved Memories\n'));

    memories.forEach((m, i) => {
      const type = m.sourceType === 'context' ? 'ðŸ“‹' : 'ðŸ’¬';
      const similarity = (m.similarity * 100).toFixed(0);

      console.log(`${type} ${chalk.cyan(`[${similarity}%]`)} ${m.content.slice(0, 150)}...`);
      console.log();
    });
  });
}
```

---

## Tasks Breakdown

| Task | Description | Estimate |
|------|-------------|----------|
| 3.3.1 | Create MemoryRetriever class | 45 min |
| 3.3.2 | Implement query embedding | 20 min |
| 3.3.3 | Add relevance filtering | 30 min |
| 3.3.4 | Implement deduplication | 20 min |
| 3.3.5 | Add token-aware truncation | 30 min |
| 3.3.6 | Format memories for prompt | 30 min |
| 3.3.7 | Create ChatWithMemoryService | 45 min |
| 3.3.8 | Implement /recall command | 30 min |
| 3.3.9 | Integrate with chat flow | 30 min |
| 3.3.10 | Write integration tests | 45 min |

---

## Acceptance Criteria

- [ ] Retrieval finds relevant memories
- [ ] Context injected into prompts
- [ ] Deduplication works
- [ ] Token limits respected
- [ ] /recall shows results
- [ ] Retrieval < 200ms
- [ ] AI references past discussions
- [ ] Threshold filters noise

---

## Definition of Done

- [ ] All tasks completed
- [ ] All acceptance criteria met
- [ ] End-to-end tested
- [ ] Performance verified

