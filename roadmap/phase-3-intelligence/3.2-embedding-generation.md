# Feature 3.2: Embedding Generation

## Metadata
| Field | Value |
|-------|-------|
| **Feature ID** | 3.2 |
| **Phase** | 3 - Intelligence |
| **Priority** | Critical |
| **Estimated Effort** | 4-5 hours |
| **Dependencies** | 3.1 (pgvector Setup), 1.2 (LLM Integration) |
| **Approval** | [ ] |
| **Status** | Not Started |

---

## Overview

Implement embedding generation for messages and context using OpenAI's embedding API, with caching and batch processing for efficiency.

## User Story

As a **system**, I need to **convert text to embeddings** so that **semantic search works**.

---

## Requirements

### Functional Requirements
1. Generate embeddings via OpenAI API
2. Support batch embedding generation
3. Cache embeddings to avoid regeneration
4. Auto-embed new messages
5. Embed business context

### Non-Functional Requirements
- Use text-embedding-3-small (cost effective)
- Batch up to 100 texts per request
- Rate limit handling
- Cost tracking

---

## Technical Specification

### Embedding Service

```typescript
// src/infrastructure/embedding/embedding.service.ts
import OpenAI from 'openai';

export interface EmbeddingOptions {
  model?: string;
  dimensions?: number;
}

export class EmbeddingService {
  private client: OpenAI;
  private model: string;
  private dimensions: number;
  private cache: Map<string, number[]> = new Map();

  constructor(config: { apiKey: string; model?: string; dimensions?: number }) {
    this.client = new OpenAI({ apiKey: config.apiKey });
    this.model = config.model || 'text-embedding-3-small';
    this.dimensions = config.dimensions || 1536;
  }

  async embed(text: string): Promise<number[]> {
    // Check cache
    const cacheKey = this.getCacheKey(text);
    if (this.cache.has(cacheKey)) {
      return this.cache.get(cacheKey)!;
    }

    const response = await this.client.embeddings.create({
      model: this.model,
      input: text,
      dimensions: this.dimensions,
    });

    const embedding = response.data[0].embedding;

    // Cache result
    this.cache.set(cacheKey, embedding);

    return embedding;
  }

  async embedBatch(texts: string[]): Promise<number[][]> {
    if (texts.length === 0) return [];
    if (texts.length === 1) return [await this.embed(texts[0])];

    // Check cache for all
    const results: (number[] | null)[] = texts.map(t => {
      const key = this.getCacheKey(t);
      return this.cache.get(key) || null;
    });

    // Find uncached
    const uncachedIndices = results
      .map((r, i) => (r === null ? i : -1))
      .filter(i => i !== -1);

    if (uncachedIndices.length === 0) {
      return results as number[][];
    }

    // Batch embed uncached (max 100 per request)
    const uncachedTexts = uncachedIndices.map(i => texts[i]);
    const batches = this.chunk(uncachedTexts, 100);

    let embeddings: number[][] = [];

    for (const batch of batches) {
      const response = await this.client.embeddings.create({
        model: this.model,
        input: batch,
        dimensions: this.dimensions,
      });

      const batchEmbeddings = response.data
        .sort((a, b) => a.index - b.index)
        .map(d => d.embedding);

      embeddings = [...embeddings, ...batchEmbeddings];
    }

    // Cache and fill results
    uncachedIndices.forEach((originalIndex, i) => {
      const embedding = embeddings[i];
      const text = texts[originalIndex];
      this.cache.set(this.getCacheKey(text), embedding);
      results[originalIndex] = embedding;
    });

    return results as number[][];
  }

  async embedWithMetadata(text: string): Promise<{
    embedding: number[];
    tokens: number;
    model: string;
  }> {
    const response = await this.client.embeddings.create({
      model: this.model,
      input: text,
      dimensions: this.dimensions,
    });

    return {
      embedding: response.data[0].embedding,
      tokens: response.usage.total_tokens,
      model: this.model,
    };
  }

  clearCache(): void {
    this.cache.clear();
  }

  getCacheSize(): number {
    return this.cache.size;
  }

  private getCacheKey(text: string): string {
    // Simple hash - in production use proper hash
    return `${this.model}:${text.slice(0, 100)}:${text.length}`;
  }

  private chunk<T>(array: T[], size: number): T[][] {
    const chunks: T[][] = [];
    for (let i = 0; i < array.length; i += size) {
      chunks.push(array.slice(i, i + size));
    }
    return chunks;
  }
}
```

### Message Embedder

```typescript
// src/core/memory/message-embedder.ts
import { EmbeddingService } from '../../infrastructure/embedding/embedding.service';
import { VectorService } from '../vector/vector.service';
import { Message } from '../../types';

export class MessageEmbedder {
  private pendingMessages: Message[] = [];
  private batchTimeout: NodeJS.Timeout | null = null;
  private readonly BATCH_DELAY = 5000; // 5 seconds
  private readonly BATCH_SIZE = 20;

  constructor(
    private embeddingService: EmbeddingService,
    private vectorService: VectorService
  ) {}

  async embedMessage(message: Message): Promise<void> {
    // Skip system messages
    if (message.role === 'system') return;

    // Add to pending batch
    this.pendingMessages.push(message);

    // Process immediately if batch is full
    if (this.pendingMessages.length >= this.BATCH_SIZE) {
      await this.processPending();
    } else {
      // Schedule batch processing
      this.scheduleBatch();
    }
  }

  async embedMessageImmediate(message: Message): Promise<void> {
    if (message.role === 'system') return;

    const embedding = await this.embeddingService.embed(message.content);

    await this.vectorService.store(
      'message',
      message.id,
      message.content,
      embedding
    );
  }

  private scheduleBatch(): void {
    if (this.batchTimeout) return;

    this.batchTimeout = setTimeout(async () => {
      await this.processPending();
    }, this.BATCH_DELAY);
  }

  private async processPending(): Promise<void> {
    if (this.batchTimeout) {
      clearTimeout(this.batchTimeout);
      this.batchTimeout = null;
    }

    if (this.pendingMessages.length === 0) return;

    const messages = [...this.pendingMessages];
    this.pendingMessages = [];

    try {
      const texts = messages.map(m => m.content);
      const embeddings = await this.embeddingService.embedBatch(texts);

      const items = messages.map((msg, i) => ({
        sourceType: 'message' as const,
        sourceId: msg.id,
        content: msg.content,
        embedding: embeddings[i],
      }));

      await this.vectorService.storeBatch(items);
    } catch (error) {
      console.error('Failed to embed messages:', error);
      // Re-queue failed messages
      this.pendingMessages = [...messages, ...this.pendingMessages];
    }
  }

  async flush(): Promise<void> {
    await this.processPending();
  }
}
```

### Context Embedder

```typescript
// src/core/memory/context-embedder.ts
import { EmbeddingService } from '../../infrastructure/embedding/embedding.service';
import { VectorService } from '../vector/vector.service';
import { BusinessContext } from '../../types';

export class ContextEmbedder {
  constructor(
    private embeddingService: EmbeddingService,
    private vectorService: VectorService
  ) {}

  async embedContext(businessId: string, context: BusinessContext): Promise<void> {
    const segments = this.segmentContext(context);

    for (const segment of segments) {
      const embedding = await this.embeddingService.embed(segment.text);

      await this.vectorService.store(
        'context',
        `${businessId}:${segment.key}`,
        segment.text,
        embedding
      );
    }
  }

  private segmentContext(context: BusinessContext): Array<{ key: string; text: string }> {
    const segments: Array<{ key: string; text: string }> = [];

    if (context.problemStatement) {
      segments.push({
        key: 'problem',
        text: `Problem: ${context.problemStatement}`,
      });
    }

    if (context.targetCustomer) {
      segments.push({
        key: 'target',
        text: `Target Customer: ${context.targetCustomer}`,
      });
    }

    if (context.challenges.length > 0) {
      segments.push({
        key: 'challenges',
        text: `Challenges: ${context.challenges.join('. ')}`,
      });
    }

    if (context.goals.length > 0) {
      segments.push({
        key: 'goals',
        text: `Goals: ${context.goals.join('. ')}`,
      });
    }

    return segments;
  }
}
```

---

## Tasks Breakdown

| Task | Description | Estimate |
|------|-------------|----------|
| 3.2.1 | Create EmbeddingService | 45 min |
| 3.2.2 | Implement single embed | 20 min |
| 3.2.3 | Implement batch embed | 30 min |
| 3.2.4 | Add in-memory cache | 20 min |
| 3.2.5 | Create MessageEmbedder | 45 min |
| 3.2.6 | Add batch processing | 30 min |
| 3.2.7 | Create ContextEmbedder | 30 min |
| 3.2.8 | Integrate with message flow | 30 min |
| 3.2.9 | Add token/cost tracking | 20 min |
| 3.2.10 | Write unit tests | 30 min |

---

## Acceptance Criteria

- [ ] Single text embedding works
- [ ] Batch embedding works
- [ ] Cache prevents re-embedding
- [ ] Messages auto-embed
- [ ] Context embeds on update
- [ ] Rate limits handled
- [ ] Batch processing efficient
- [ ] Token usage tracked

---

## Cost Estimation

```
text-embedding-3-small: $0.02 / 1M tokens

Avg message: ~100 tokens
1000 messages/month = 100K tokens = $0.002

Very cost effective for semantic search.
```

---

## Definition of Done

- [ ] All tasks completed
- [ ] All acceptance criteria met
- [ ] Cost tracking verified
- [ ] Integration tested

