# Feature 3.5: Progressive Disclosure (CLI)

## Metadata
| Field | Value |
|-------|-------|
| **Feature ID** | CLI-3.5 |
| **Phase** | 3 - Intelligence & Vector Search (CLI) |
| **Priority** | High |
| **Estimated Effort** | 3-4 hours |
| **Dependencies** | CLI-3.4 (Relevance Scoring) |
| **Approval** | [ ] |
| **Status** | Not Started |

---

## Overview

Implement 4-level progressive context disclosure to optimize token usage: start with minimal context, progressively add detail only when needed based on query complexity and confidence.

## User Story

As a **CLI user**, I want **the AI to use tokens efficiently** so that **I get fast responses with lower costs while maintaining quality**.

---

## Requirements

### Functional
1. 4-level context hierarchy (L1-L4)
2. Automatic level selection based on query complexity
3. Confidence-based escalation (< 8.5 → add more context)
4. Token budget management (target: < 4K tokens)
5. Cache frequently used context

### Non-Functional
- Context assembly < 100ms
- Token savings: 30-50% vs full context
- Maintain quality (confidence > 8.5)
- Graceful degradation if token limit hit

---

## Technical Specification

```typescript
// src/core/intelligence/progressive-disclosure.service.ts
export enum ContextLevel {
  L1_MINIMAL = 1,      // Business name, stage, current question only
  L2_SUMMARY = 2,      // + Business context summary, recent 3 messages
  L3_RELEVANT = 3,     // + Top 5 relevant past messages
  L4_COMPREHENSIVE = 4 // + Full conversation history (last 20 messages)
}

export interface ContextBudget {
  maxTokens: number;
  reservedForResponse: number;
  availableForContext: number;
}

export interface ContextPackage {
  level: ContextLevel;
  tokens: number;
  content: string;
  sources: string[];
}

export class ProgressiveDisclosureService {
  private readonly TOKEN_BUDGETS = {
    [ContextLevel.L1_MINIMAL]: 500,
    [ContextLevel.L2_SUMMARY]: 1500,
    [ContextLevel.L3_RELEVANT]: 3000,
    [ContextLevel.L4_COMPREHENSIVE]: 6000
  };

  constructor(
    private retrieval: RetrievalService,
    private scoring: ScoringService,
    private businessRepo: BusinessRepository,
    private conversationRepo: ConversationRepository,
    private messageRepo: MessageRepository,
    private tokenCounter: TokenCounter
  ) {}

  async buildContext(
    conversationId: string,
    currentMessage: string,
    budget: ContextBudget = {
      maxTokens: 8000,
      reservedForResponse: 2000,
      availableForContext: 6000
    }
  ): Promise<ContextPackage> {
    // Determine initial level based on query complexity
    const initialLevel = this.determineInitialLevel(currentMessage);

    // Build context at that level
    let contextPkg = await this.buildContextAtLevel(
      conversationId,
      currentMessage,
      initialLevel,
      budget
    );

    return contextPkg;
  }

  async buildContextAtLevel(
    conversationId: string,
    currentMessage: string,
    level: ContextLevel,
    budget: ContextBudget
  ): Promise<ContextPackage> {
    const conversation = await this.conversationRepo.findById(conversationId);
    if (!conversation) {
      throw new NotFoundError('Conversation not found');
    }

    const business = await this.businessRepo.findById(conversation.business_id);
    if (!business) {
      throw new NotFoundError('Business not found');
    }

    const parts: string[] = [];
    const sources: string[] = [];

    switch (level) {
      case ContextLevel.L1_MINIMAL:
        parts.push(this.buildL1Context(business));
        sources.push('business_basic');
        break;

      case ContextLevel.L2_SUMMARY:
        parts.push(this.buildL2Context(business));
        const recentMsgs = await this.getRecentMessages(conversationId, 3);
        if (recentMsgs.length > 0) {
          parts.push('\n## Recent Messages');
          parts.push(this.formatMessages(recentMsgs));
          sources.push('business_context', 'recent_messages');
        }
        break;

      case ContextLevel.L3_RELEVANT:
        parts.push(this.buildL2Context(business));

        // Get relevant past messages
        const relevantResults = await this.retrieval.searchWithScoring(currentMessage, {
          businessId: business.id,
          currentBusinessId: business.id,
          currentStage: business.stage,
          limit: 5,
          threshold: 0.7
        });

        if (relevantResults.length > 0) {
          parts.push('\n## Relevant Past Discussions');
          parts.push(this.formatScoredResults(relevantResults));
          sources.push('business_context', 'semantic_search');
        }
        break;

      case ContextLevel.L4_COMPREHENSIVE:
        parts.push(this.buildL2Context(business));

        // Get full conversation history
        const allMessages = await this.messageRepo.findByConversationId(conversationId);
        const last20 = allMessages.slice(-20);

        if (last20.length > 0) {
          parts.push('\n## Conversation History');
          parts.push(this.formatMessages(last20));
          sources.push('business_context', 'full_history');
        }

        // Also add relevant cross-conversation context
        const crossConv = await this.retrieval.searchWithScoring(currentMessage, {
          businessId: business.id,
          currentBusinessId: business.id,
          currentStage: business.stage,
          limit: 3,
          threshold: 0.75
        });

        if (crossConv.length > 0) {
          parts.push('\n## Related Discussions (Other Conversations)');
          parts.push(this.formatScoredResults(crossConv));
          sources.push('cross_conversation');
        }
        break;
    }

    const content = parts.join('\n');
    const tokens = this.tokenCounter.count(content);

    // If over budget, try to trim
    if (tokens > budget.availableForContext && level > ContextLevel.L1_MINIMAL) {
      // Fall back to lower level
      return this.buildContextAtLevel(
        conversationId,
        currentMessage,
        (level - 1) as ContextLevel,
        budget
      );
    }

    return {
      level,
      tokens,
      content,
      sources
    };
  }

  private determineInitialLevel(message: string): ContextLevel {
    const messageLength = message.length;
    const wordCount = message.split(/\s+/).length;

    // Simple heuristic for query complexity
    if (wordCount < 10 && !this.hasComplexKeywords(message)) {
      return ContextLevel.L1_MINIMAL; // Simple question
    } else if (wordCount < 30) {
      return ContextLevel.L2_SUMMARY; // Medium complexity
    } else {
      return ContextLevel.L3_RELEVANT; // Complex question needs more context
    }
  }

  private hasComplexKeywords(message: string): boolean {
    const complexKeywords = [
      'compare', 'analyze', 'detailed', 'comprehensive',
      'history', 'explain', 'strategy', 'plan'
    ];

    const lowerMessage = message.toLowerCase();
    return complexKeywords.some(kw => lowerMessage.includes(kw));
  }

  private buildL1Context(business: Business): string {
    return `Business: ${business.name} (${business.stage} stage)`;
  }

  private buildL2Context(business: Business): string {
    const parts = [
      `## Business Context`,
      `Name: ${business.name}`,
      `Industry: ${business.industry}`,
      `Stage: ${business.stage}`,
    ];

    if (business.context.targetCustomer) {
      parts.push(`Target Customer: ${business.context.targetCustomer}`);
    }

    if (business.context.problemStatement) {
      parts.push(`Problem: ${business.context.problemStatement}`);
    }

    if (business.context.goals.length > 0) {
      parts.push(`Goals: ${business.context.goals.join(', ')}`);
    }

    return parts.join('\n');
  }

  private async getRecentMessages(
    conversationId: string,
    limit: number
  ): Promise<Message[]> {
    const allMessages = await this.messageRepo.findByConversationId(conversationId);
    return allMessages.slice(-limit);
  }

  private formatMessages(messages: Message[]): string {
    return messages
      .map(m => `[${m.role}]: ${m.content}`)
      .join('\n\n');
  }

  private formatScoredResults(results: ScoredResult[]): string {
    return results
      .map(r => {
        const preview = r.content.slice(0, 200);
        return `- ${preview}... (score: ${r.scores.total.toFixed(2)}, ${r.explanation})`;
      })
      .join('\n\n');
  }

  async shouldEscalateLevel(
    currentLevel: ContextLevel,
    responseConfidence: number
  ): Promise<boolean> {
    // If confidence < 8.5, consider escalating
    const CONFIDENCE_THRESHOLD = 8.5;

    return (
      responseConfidence < CONFIDENCE_THRESHOLD &&
      currentLevel < ContextLevel.L4_COMPREHENSIVE
    );
  }
}
```

### Token Counter Utility

```typescript
// src/core/utils/token-counter.ts
import { encoding_for_model } from 'tiktoken';

export class TokenCounter {
  private encoder;

  constructor(model: string = 'gpt-4') {
    this.encoder = encoding_for_model(model);
  }

  count(text: string): number {
    const tokens = this.encoder.encode(text);
    return tokens.length;
  }

  countMessages(messages: any[]): number {
    // More accurate counting for chat messages
    let total = 0;
    for (const msg of messages) {
      total += 4; // Message overhead
      total += this.count(msg.role);
      total += this.count(msg.content);
    }
    total += 2; // Conversation overhead
    return total;
  }

  truncate(text: string, maxTokens: number): string {
    const tokens = this.encoder.encode(text);
    if (tokens.length <= maxTokens) {
      return text;
    }

    const truncated = tokens.slice(0, maxTokens);
    return this.encoder.decode(truncated);
  }
}
```

### Integration with Chat Service

```typescript
// Update src/core/chat/chat.service.ts
export class ChatService {
  constructor(
    private progressiveDisclosure: ProgressiveDisclosureService,
    // ... other dependencies
  ) {}

  async sendMessage(conversationId: string, content: string): Promise<void> {
    // Build context progressively
    const contextPkg = await this.progressiveDisclosure.buildContext(
      conversationId,
      content,
      {
        maxTokens: 8000,
        reservedForResponse: 2000,
        availableForContext: 6000
      }
    );

    console.log(chalk.gray(`Using context level L${contextPkg.level} (${contextPkg.tokens} tokens)`));

    // Send to LLM with context
    const response = await this.llm.chat([
      { role: 'system', content: this.buildSystemPrompt(contextPkg.content) },
      { role: 'user', content }
    ]);

    // Check confidence
    const confidence = this.extractConfidence(response);

    if (await this.progressiveDisclosure.shouldEscalateLevel(contextPkg.level, confidence)) {
      console.log(chalk.yellow('Low confidence detected. Retrying with more context...'));

      // Escalate to next level
      const enhancedPkg = await this.progressiveDisclosure.buildContextAtLevel(
        conversationId,
        content,
        (contextPkg.level + 1) as ContextLevel,
        { maxTokens: 8000, reservedForResponse: 2000, availableForContext: 6000 }
      );

      const retryResponse = await this.llm.chat([
        { role: 'system', content: this.buildSystemPrompt(enhancedPkg.content) },
        { role: 'user', content }
      ]);

      await this.messageRepo.create({
        conversationId,
        role: 'assistant',
        content: retryResponse.content
      });
    } else {
      await this.messageRepo.create({
        conversationId,
        role: 'assistant',
        content: response.content
      });
    }
  }
}
```

---

## Tasks

| Task | Estimate |
|------|----------|
| Create ProgressiveDisclosureService | 60 min |
| Implement 4 context levels | 45 min |
| Add TokenCounter utility | 20 min |
| Implement level escalation | 30 min |
| Integrate with ChatService | 30 min |
| Add token budget management | 20 min |
| Write tests | 30 min |

---

## Acceptance Criteria
- [ ] 4 context levels work (L1-L4)
- [ ] Initial level auto-selected
- [ ] Token budgets respected
- [ ] Escalation on low confidence
- [ ] 30-50% token savings vs full context
- [ ] Response quality maintained (confidence > 8.5)

---

## Expected Token Savings

**Baseline (no progressive disclosure)**: 6,000 tokens avg per request

**With progressive disclosure**:
- 60% queries: L1-L2 (500-1500 tokens) → Save 70-75%
- 30% queries: L3 (3000 tokens) → Save 50%
- 10% queries: L4 (6000 tokens) → Save 0%

**Average savings**: ~55% reduction in context tokens

---

## Dependencies

```json
{
  "tiktoken": "^1.0.7"
}
```
