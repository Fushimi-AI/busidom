# Feature 3.2: Embedding Generation (CLI)

## Metadata
| Field | Value |
|-------|-------|
| **Feature ID** | CLI-3.2 |
| **Phase** | 3 - Intelligence & Vector Search (CLI) |
| **Priority** | Critical |
| **Estimated Effort** | 3-4 hours |
| **Dependencies** | CLI-3.1 (Vector Setup), CLI-2.3 (Conversation Storage) |
| **Approval** | [ ] |
| **Status** | Not Started |

---

## Overview

Generate vector embeddings for all user messages, business context, and conversation history to enable semantic search and intelligent context retrieval.

## User Story

As a **CLI user**, I want **my conversations and business context automatically embedded** so that **the AI can intelligently retrieve relevant past information**.

---

## Requirements

### Functional
1. Generate embeddings for all new messages
2. Generate embeddings for business context updates
3. Support batch embedding generation
4. Cache embeddings to avoid redundant API calls
5. Handle embedding API failures gracefully

### Non-Functional
- Embedding generation < 500ms per message
- Support up to 8,000 tokens per chunk
- 99% embedding success rate
- Cost-effective (use small model: text-embedding-3-small)

---

## Technical Specification

```typescript
// src/core/intelligence/embedding.service.ts
import OpenAI from 'openai';

export interface EmbeddingMetadata {
  sourceType: 'message' | 'business_context' | 'workflow';
  sourceId: string;
  content: string;
  timestamp: number;
}

export class EmbeddingService {
  private openai: OpenAI;
  private cache: Map<string, number[]> = new Map();

  constructor(
    private config: Config,
    private vectorStore: IVectorStore
  ) {
    this.openai = new OpenAI({ apiKey: config.openaiApiKey });
  }

  async generateEmbedding(text: string): Promise<number[]> {
    // Check cache first (hash-based)
    const cacheKey = this.hashText(text);
    if (this.cache.has(cacheKey)) {
      return this.cache.get(cacheKey)!;
    }

    try {
      const response = await this.openai.embeddings.create({
        model: 'text-embedding-3-small',
        input: text,
        encoding_format: 'float'
      });

      const embedding = response.data[0].embedding;
      this.cache.set(cacheKey, embedding);
      return embedding;
    } catch (error) {
      console.error('Embedding generation failed:', error);
      throw new EmbeddingError('Failed to generate embedding', { cause: error });
    }
  }

  async generateBatch(texts: string[]): Promise<number[][]> {
    // OpenAI supports batch embedding (up to 2048 inputs)
    const BATCH_SIZE = 100;
    const results: number[][] = [];

    for (let i = 0; i < texts.length; i += BATCH_SIZE) {
      const batch = texts.slice(i, i + BATCH_SIZE);

      const response = await this.openai.embeddings.create({
        model: 'text-embedding-3-small',
        input: batch,
        encoding_format: 'float'
      });

      results.push(...response.data.map(d => d.embedding));
    }

    return results;
  }

  async embedMessage(message: Message): Promise<void> {
    const embedding = await this.generateEmbedding(message.content);

    await this.vectorStore.upsert({
      id: message.id,
      sourceType: 'message',
      sourceId: message.id,
      content: message.content,
      embedding,
      metadata: {
        conversationId: message.conversation_id,
        role: message.role,
        timestamp: message.created_at
      }
    });
  }

  async embedBusinessContext(business: Business): Promise<void> {
    // Combine business context into searchable text
    const contextText = this.formatBusinessContext(business);
    const embedding = await this.generateEmbedding(contextText);

    await this.vectorStore.upsert({
      id: `business_${business.id}`,
      sourceType: 'business_context',
      sourceId: business.id,
      content: contextText,
      embedding,
      metadata: {
        businessId: business.id,
        name: business.name,
        stage: business.stage,
        timestamp: business.updated_at
      }
    });
  }

  private formatBusinessContext(business: Business): string {
    const parts = [
      `Business: ${business.name}`,
      `Industry: ${business.industry}`,
      `Stage: ${business.stage}`,
    ];

    if (business.context.targetCustomer) {
      parts.push(`Target Customer: ${business.context.targetCustomer}`);
    }

    if (business.context.problemStatement) {
      parts.push(`Problem: ${business.context.problemStatement}`);
    }

    if (business.context.challenges.length > 0) {
      parts.push(`Challenges: ${business.context.challenges.join(', ')}`);
    }

    if (business.context.goals.length > 0) {
      parts.push(`Goals: ${business.context.goals.join(', ')}`);
    }

    return parts.join('\n');
  }

  private hashText(text: string): string {
    // Simple hash for caching (crypto.createHash in production)
    return text.slice(0, 100);
  }
}
```

### Background Embedding Worker

```typescript
// src/core/intelligence/embedding.worker.ts
export class EmbeddingWorker {
  constructor(
    private embeddingService: EmbeddingService,
    private messageRepo: MessageRepository,
    private businessRepo: BusinessRepository
  ) {}

  async processQueue(): Promise<void> {
    // Find messages without embeddings
    const unembeddedMessages = await this.messageRepo.findWithoutEmbeddings();

    for (const message of unembeddedMessages) {
      try {
        await this.embeddingService.embedMessage(message);
        console.log(`Embedded message ${message.id}`);
      } catch (error) {
        console.error(`Failed to embed message ${message.id}:`, error);
      }
    }

    // Find business contexts that need re-embedding (updated_at changed)
    const businesses = await this.businessRepo.findAll();
    for (const business of businesses) {
      const lastEmbedded = await this.getLastEmbeddedTimestamp(business.id);
      if (!lastEmbedded || business.updated_at > lastEmbedded) {
        await this.embeddingService.embedBusinessContext(business);
      }
    }
  }

  private async getLastEmbeddedTimestamp(businessId: string): Promise<number | null> {
    // Query vector store for last embedding timestamp
    return null; // Implement based on vector store
  }
}
```

### Integration with Chat Service

```typescript
// Update src/core/chat/chat.service.ts
export class ChatService {
  async sendMessage(conversationId: string, content: string): Promise<void> {
    const message = await this.messageRepo.create({
      conversationId,
      role: 'user',
      content
    });

    // Trigger embedding generation in background
    this.embeddingService.embedMessage(message).catch(err => {
      console.error('Background embedding failed:', err);
    });

    // Continue with chat response...
  }
}
```

---

## Tasks

| Task | Estimate |
|------|----------|
| Create EmbeddingService | 45 min |
| Implement batch generation | 30 min |
| Add caching layer | 20 min |
| Create background worker | 45 min |
| Integrate with ChatService | 30 min |
| Handle API failures | 20 min |
| Write tests | 30 min |

---

## Acceptance Criteria
- [ ] Messages auto-embedded on creation
- [ ] Business context embedded on update
- [ ] Batch embedding works for 100+ items
- [ ] Cache reduces redundant API calls
- [ ] Embedding failures don't break chat
- [ ] Background worker processes queue

---

## Dependencies

```json
{
  "openai": "^4.20.0"
}
```

---

## Cost Considerations

**text-embedding-3-small pricing**: $0.02 per 1M tokens

Estimated usage:
- 1,000 messages/month Ã— 200 tokens avg = 200K tokens
- Cost: $0.004/month per user

For CLI (local), users bring their own API key, so this is transparent.
