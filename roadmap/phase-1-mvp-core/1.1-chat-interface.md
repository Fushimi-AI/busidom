# Feature 1.1: Chat Interface

## Metadata
| Field | Value |
|-------|-------|
| **Feature ID** | 1.1 |
| **Phase** | 1 - MVP Core |
| **Priority** | Critical |
| **Estimated Effort** | 4-6 hours |
| **Dependencies** | 0.4 (Basic CLI), 0.5 (Configuration) |
| **Approval** | [ ] |
| **Status** | Not Started |

---

## Overview

Implement the core chat interface that handles user input, manages conversation flow, and displays AI responses with streaming support.

## User Story

As a **founder**, I want to **chat with my AI co-founder** so that **I can get guidance on my business decisions**.

---

## Requirements

### Functional Requirements
1. Send user messages to AI
2. Display AI responses with streaming
3. Handle conversation context
4. Support multi-turn conversations
5. Display thinking/processing indicators
6. Handle API errors gracefully

### Non-Functional Requirements
- Response streaming with < 100ms first token
- Smooth text rendering (no flickering)
- Graceful degradation if streaming fails
- Maximum 30 second timeout

---

## Technical Specification

### Chat Service Interface

```typescript
// src/core/chat/chat.service.ts
import { ILLMService } from '../interfaces/llm.interface';
import { Message, Result } from '../../types';

export interface ChatOptions {
  stream?: boolean;
  maxTokens?: number;
  temperature?: number;
}

export class ChatService {
  private conversationHistory: Message[] = [];

  constructor(
    private llm: ILLMService,
    private systemPrompt: string
  ) {}

  async send(userMessage: string, options?: ChatOptions): Promise<Result<Message>> {
    const message: Message = {
      id: generateId(),
      role: 'user',
      content: userMessage,
      timestamp: new Date(),
    };

    this.conversationHistory.push(message);

    const messages = [
      { role: 'system', content: this.systemPrompt },
      ...this.conversationHistory,
    ];

    const result = await this.llm.chat(messages, options);

    if (result.success) {
      this.conversationHistory.push(result.data);
    }

    return result;
  }

  async *stream(userMessage: string): AsyncIterable<string> {
    const message: Message = {
      id: generateId(),
      role: 'user',
      content: userMessage,
      timestamp: new Date(),
    };

    this.conversationHistory.push(message);

    const messages = [
      { role: 'system', content: this.systemPrompt },
      ...this.conversationHistory,
    ];

    let fullResponse = '';

    for await (const chunk of this.llm.stream(messages)) {
      fullResponse += chunk;
      yield chunk;
    }

    // Add complete response to history
    this.conversationHistory.push({
      id: generateId(),
      role: 'assistant',
      content: fullResponse,
      timestamp: new Date(),
    });
  }

  getHistory(): Message[] {
    return [...this.conversationHistory];
  }

  clearHistory(): void {
    this.conversationHistory = [];
  }
}
```

### Streaming Output Handler

```typescript
// src/cli/stream-handler.ts
import chalk from 'chalk';

export class StreamHandler {
  private buffer: string = '';

  async handleStream(stream: AsyncIterable<string>): Promise<string> {
    process.stdout.write(chalk.green('Mentor: '));

    for await (const chunk of stream) {
      process.stdout.write(chunk);
      this.buffer += chunk;
    }

    process.stdout.write('\n\n');
    return this.buffer;
  }
}
```

### Updated REPL

```typescript
// src/cli/repl.ts (updated)
export class REPL {
  private async handleMessage(input: string): Promise<void> {
    try {
      if (this.config.features.streaming) {
        const stream = this.chatService.stream(input);
        await this.streamHandler.handleStream(stream);
      } else {
        const spinner = ora('Thinking...').start();
        const result = await this.chatService.send(input);

        if (result.success) {
          spinner.stop();
          console.log(format.assistant(result.data.content));
        } else {
          spinner.fail();
          console.log(format.error(result.error.message));
        }
      }
    } catch (error) {
      console.log(format.error('Failed to get response'));
      if (this.config.features.debug) {
        console.error(error);
      }
    }
  }
}
```

### Error Handling

```typescript
// src/core/chat/errors.ts
export class ChatError extends AppError {
  constructor(message: string, public retryable: boolean = false) {
    super(message, 'CHAT_ERROR', 500);
  }
}

export class RateLimitError extends ChatError {
  constructor(retryAfter?: number) {
    super(`Rate limited. Retry after ${retryAfter}s`, true);
  }
}

export class ContextLengthError extends ChatError {
  constructor() {
    super('Conversation too long. Use /clear to start fresh.', false);
  }
}
```

---

## Tasks Breakdown

| Task | Description | Estimate |
|------|-------------|----------|
| 1.1.1 | Create ChatService class | 45 min |
| 1.1.2 | Implement send method | 30 min |
| 1.1.3 | Implement stream method | 45 min |
| 1.1.4 | Create StreamHandler for output | 30 min |
| 1.1.5 | Integrate with REPL | 30 min |
| 1.1.6 | Add error handling | 30 min |
| 1.1.7 | Implement retry logic | 30 min |
| 1.1.8 | Add timeout handling | 20 min |
| 1.1.9 | Write unit tests | 45 min |
| 1.1.10 | Integration test with mock | 30 min |

---

## Acceptance Criteria

- [ ] User can send messages
- [ ] AI responses stream in real-time
- [ ] Conversation context maintained
- [ ] Multi-turn conversations work
- [ ] Spinner shows during non-streaming
- [ ] API errors show user-friendly message
- [ ] Rate limits handled with retry
- [ ] Context length errors prompt /clear
- [ ] 30 second timeout enforced

---

## File Structure

```
src/
├── core/
│   └── chat/
│       ├── index.ts
│       ├── chat.service.ts
│       └── errors.ts
├── cli/
│   ├── repl.ts (updated)
│   └── stream-handler.ts
```

---

## Risks & Mitigations

| Risk | Impact | Mitigation |
|------|--------|------------|
| Streaming not supported | Medium | Fallback to non-streaming |
| API timeouts | Medium | 30s timeout + retry |
| Context too long | Medium | Warn user, suggest /clear |

---

## Definition of Done

- [ ] All tasks completed
- [ ] All acceptance criteria met
- [ ] Streaming works end-to-end
- [ ] Error handling tested
- [ ] Manual testing passed

