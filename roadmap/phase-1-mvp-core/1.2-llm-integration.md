# Feature 1.2: LLM Integration

## Metadata
| Field | Value |
|-------|-------|
| **Feature ID** | 1.2 |
| **Phase** | 1 - MVP Core |
| **Priority** | Critical |
| **Estimated Effort** | 5-7 hours |
| **Dependencies** | 0.3 (Core Architecture), 0.5 (Configuration) |
| **Approval** | [ ] |
| **Status** | Not Started |

---

## Overview

Implement the LLM service layer supporting multiple providers (OpenAI, Kimi, Anthropic) with a unified interface, streaming support, and proper error handling.

## User Story

As a **developer**, I want a **provider-agnostic LLM service** so that **users can choose their preferred AI provider**.

---

## Requirements

### Functional Requirements
1. Support OpenAI API
2. Support Kimi API (OpenAI-compatible)
3. Support Anthropic API
4. Unified interface for all providers
5. Streaming and non-streaming modes
6. Token counting and limits
7. Retry with exponential backoff

### Non-Functional Requirements
- Provider switch without code changes
- < 200ms overhead per request
- Proper timeout handling
- No API keys in logs

---

## Technical Specification

### LLM Service Interface

```typescript
// src/core/interfaces/llm.interface.ts
import { Message, Result } from '../../types';

export interface ChatOptions {
  model?: string;
  maxTokens?: number;
  temperature?: number;
  stopSequences?: string[];
  timeout?: number;
}

export interface ILLMService {
  chat(messages: Message[], options?: ChatOptions): Promise<Result<Message>>;
  stream(messages: Message[], options?: ChatOptions): AsyncIterable<string>;
  countTokens(text: string): number;
  getModel(): string;
}
```

### OpenAI Provider

```typescript
// src/infrastructure/llm/openai.provider.ts
import OpenAI from 'openai';
import { ILLMService, ChatOptions } from '../../core/interfaces/llm.interface';
import { Message, Result } from '../../types';
import { LLMError } from '../../core/errors';

export class OpenAIProvider implements ILLMService {
  private client: OpenAI;
  private model: string;

  constructor(config: { apiKey: string; baseUrl?: string; model: string }) {
    this.client = new OpenAI({
      apiKey: config.apiKey,
      baseURL: config.baseUrl,
    });
    this.model = config.model;
  }

  async chat(messages: Message[], options?: ChatOptions): Promise<Result<Message>> {
    try {
      const response = await this.client.chat.completions.create({
        model: options?.model || this.model,
        messages: messages.map(m => ({
          role: m.role,
          content: m.content,
        })),
        max_tokens: options?.maxTokens,
        temperature: options?.temperature,
      });

      const content = response.choices[0]?.message?.content || '';

      return {
        success: true,
        data: {
          id: response.id,
          role: 'assistant',
          content,
          timestamp: new Date(),
          metadata: {
            model: response.model,
            tokens: response.usage,
          },
        },
      };
    } catch (error) {
      return {
        success: false,
        error: this.handleError(error),
      };
    }
  }

  async *stream(messages: Message[], options?: ChatOptions): AsyncIterable<string> {
    const stream = await this.client.chat.completions.create({
      model: options?.model || this.model,
      messages: messages.map(m => ({
        role: m.role,
        content: m.content,
      })),
      max_tokens: options?.maxTokens,
      temperature: options?.temperature,
      stream: true,
    });

    for await (const chunk of stream) {
      const content = chunk.choices[0]?.delta?.content;
      if (content) {
        yield content;
      }
    }
  }

  countTokens(text: string): number {
    // Approximate: 1 token ≈ 4 characters
    return Math.ceil(text.length / 4);
  }

  getModel(): string {
    return this.model;
  }

  private handleError(error: unknown): LLMError {
    if (error instanceof OpenAI.APIError) {
      if (error.status === 429) {
        return new LLMError('Rate limited. Please wait and try again.');
      }
      if (error.status === 401) {
        return new LLMError('Invalid API key. Check your configuration.');
      }
      if (error.status === 400 && error.message.includes('context')) {
        return new LLMError('Conversation too long. Use /clear.');
      }
    }
    return new LLMError('Failed to get AI response');
  }
}
```

### Anthropic Provider

```typescript
// src/infrastructure/llm/anthropic.provider.ts
import Anthropic from '@anthropic-ai/sdk';
import { ILLMService, ChatOptions } from '../../core/interfaces/llm.interface';

export class AnthropicProvider implements ILLMService {
  private client: Anthropic;
  private model: string;

  constructor(config: { apiKey: string; model: string }) {
    this.client = new Anthropic({ apiKey: config.apiKey });
    this.model = config.model;
  }

  async chat(messages: Message[], options?: ChatOptions): Promise<Result<Message>> {
    // Extract system message
    const systemMsg = messages.find(m => m.role === 'system');
    const chatMessages = messages.filter(m => m.role !== 'system');

    const response = await this.client.messages.create({
      model: options?.model || this.model,
      max_tokens: options?.maxTokens || 4096,
      system: systemMsg?.content,
      messages: chatMessages.map(m => ({
        role: m.role as 'user' | 'assistant',
        content: m.content,
      })),
    });

    // ... handle response
  }

  async *stream(messages: Message[], options?: ChatOptions): AsyncIterable<string> {
    // Similar with stream: true
  }
}
```

### Provider Factory

```typescript
// src/infrastructure/llm/factory.ts
import { ILLMService } from '../../core/interfaces/llm.interface';
import { OpenAIProvider } from './openai.provider';
import { AnthropicProvider } from './anthropic.provider';
import { Config } from '../../config';

export function createLLMService(config: Config['llm']): ILLMService {
  switch (config.provider) {
    case 'openai':
    case 'kimi':
      return new OpenAIProvider({
        apiKey: config.apiKey,
        baseUrl: config.baseUrl,
        model: config.model,
      });

    case 'anthropic':
      return new AnthropicProvider({
        apiKey: config.apiKey,
        model: config.model,
      });

    default:
      throw new ConfigError(`Unknown LLM provider: ${config.provider}`);
  }
}
```

### Retry Logic

```typescript
// src/infrastructure/llm/retry.ts
export async function withRetry<T>(
  fn: () => Promise<T>,
  options: {
    maxRetries?: number;
    baseDelay?: number;
    maxDelay?: number;
  } = {}
): Promise<T> {
  const { maxRetries = 3, baseDelay = 1000, maxDelay = 10000 } = options;

  let lastError: Error;

  for (let attempt = 0; attempt <= maxRetries; attempt++) {
    try {
      return await fn();
    } catch (error) {
      lastError = error as Error;

      if (attempt === maxRetries) break;

      // Only retry on retryable errors
      if (!isRetryable(error)) throw error;

      const delay = Math.min(baseDelay * Math.pow(2, attempt), maxDelay);
      await sleep(delay);
    }
  }

  throw lastError!;
}

function isRetryable(error: unknown): boolean {
  if (error instanceof OpenAI.APIError) {
    return error.status === 429 || error.status >= 500;
  }
  return false;
}
```

---

## Tasks Breakdown

| Task | Description | Estimate |
|------|-------------|----------|
| 1.2.1 | Define ILLMService interface | 20 min |
| 1.2.2 | Implement OpenAIProvider | 60 min |
| 1.2.3 | Implement OpenAI streaming | 45 min |
| 1.2.4 | Implement AnthropicProvider | 60 min |
| 1.2.5 | Create provider factory | 30 min |
| 1.2.6 | Add retry logic | 30 min |
| 1.2.7 | Add error handling | 30 min |
| 1.2.8 | Add token counting | 20 min |
| 1.2.9 | Write unit tests with mocks | 60 min |
| 1.2.10 | Integration test (real API) | 30 min |

---

## Acceptance Criteria

- [ ] OpenAI provider works
- [ ] Kimi provider works (via OpenAI compat)
- [ ] Anthropic provider works
- [ ] Streaming works for all providers
- [ ] Provider selected via config
- [ ] Retry on 429/5xx errors
- [ ] Clear errors on auth failure
- [ ] No API keys logged
- [ ] Token counting works

---

## Dependencies (npm)

```json
{
  "dependencies": {
    "openai": "^4.0.0",
    "@anthropic-ai/sdk": "^0.9.0"
  }
}
```

---

## File Structure

```
src/
├── core/
│   └── interfaces/
│       └── llm.interface.ts
└── infrastructure/
    └── llm/
        ├── index.ts
        ├── factory.ts
        ├── openai.provider.ts
        ├── anthropic.provider.ts
        └── retry.ts
```

---

## Risks & Mitigations

| Risk | Impact | Mitigation |
|------|--------|------------|
| Provider API changes | Medium | Abstract behind interface |
| Rate limits | Medium | Retry with backoff |
| Provider downtime | Low | Allow switching providers |

---

## Definition of Done

- [ ] All tasks completed
- [ ] All acceptance criteria met
- [ ] All providers tested
- [ ] Streaming verified
- [ ] Error handling verified

